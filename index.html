<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>XBG: eXteroceptive Behaviour Generation </title>
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <header>
            <h1>
                XBG: End-to-end Imitation Learning for Autonomous Behaviour
                in Human-Robot Interaction and Collaboration
            </h1>
            <p>Authors: Cardenas-Perez, Carlos and Romualdi, Giulio and Elobaid, Mohammed and Dafarra, Stefano and L'erario, Giuseppe and </p>
            <p>Traversaro, Silvio and Morerio, Pietro and Del Bue, Alessio and Pucci, Daniele</p>
            <div class="buttons">
                <a href="#" class="button">PDF</a>
                <a href="#" class="button">ArXiv</a>
                <a href="#" class="button">Video</a>
                <a href="https://github.com/ami-iit/paper_cardenas_2024_ral_xbg" class="button">Code</a>
            </div>
        </header>
        
        <section id="abstract">
            <h2>Overview</h2>
            <p>
                This paper presents XBG (eXteroceptive Behaviour Generation), a multimodal end-to-end Imitation Learning (IL) system for a whole-body autonomous humanoid robot used in real-world Human-Robot Interaction (HRI) scenarios.
                The main contribution of this paper is an architecture for learning HRI behaviors using a data-driven approach. 
                Through teleoperation, a diverse dataset is collected, comprising demonstrations across multiple HRI scenarios, including handshaking, handwaving, payload reception, walking, and walking with a payload. 
                After synchronizing, filtering, and transforming the data, different Deep Neural Networks (DNN) models are trained. 
                The final system integrates different modalities comprising exteroceptive and proprioceptive sources of information to provide the robot with an understanding of its environment and its own actions. 
                The robot takes sequence of images (RGB and depth) and joints state information during the interactions and then reacts accordingly, demonstrating learned behaviors. By fusing multimodal signals in time, we encode new autonomous capabilities into the robotic platform, allowing the understanding of context changes over time. 
                The models are deployed on ergoCub, a real-world humanoid robot, and their performance is measured by calculating the success rate of the robot's behavior under the mentioned scenarios.
            </p>
            <video controls style="max-width: 100%;">
                <!-- <source src="assets/videos/xbg_video.mov" type="video/mp4"> -->
                <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/xbg_video.mov" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </section>
        
        
        <section id="methodology">
            <h2>From Teleoperation to Autonomy</h2>
            <p> 
                XBG leverages the avatar system \cite{dafarra2024icub3} that enables a human operator to embody a humanoid robotic platform. It is composed of two networks: the operator and the robot one, which are connected through diverse teleoperation and teleperception interfaces. Figure \ref{fig:xbgSystem} shows the operator (left-top) and the robot network (right) connected through the retargeted joints position, walking velocities, and the sensor feedback interface. This work introduces the embodied AI network (left-bottom) where XBG connects to the same interfaces as the operator one allowing to switch from human teleoperation to an embodied AI autonomous behavior (the XBG network architecture is presented in detail in the figure below). 
            </p>
            <div style="text-align: center;">
                <img src="assets/imgs/xbgSystem.png" alt="xbg system">
            </div>
            
        </section>
        
        <section id="XBG model">
            <h2>XBG model</h2>
            <p> </p>
            <div style="text-align: center;">
                <img src="assets/imgs/Network.png" alt="xbg neural network architecture">
            </div>
    </section>

    <section id="experiments">
        <h2>Experiments</h2>
        <h3>Handwave</h3>
        <p>The human raises both arms and wave them to greet the robot. The robot should raise the arms to greet back (In some examples only one arm was raised during the interaction).</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/handwave_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/handwave_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
        <h3>Handshake</h3>
        <p>The human approaches the robot from different angles and extends his hand to shake hands with the robot. The robot should extends one of its arms until touching the human hand to shake hands.</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/handshake_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/handshake_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
        <h3>Payload Reception</h3>
        <p>The human approaches the robot from different angles and extends a payload to the robot. The robot should extends his arms to receive the payload and then hold it until the human takes it back.</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/payload_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/payload_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
        <h3>Walk/Person following</h3>
        <p>The human walks away from the robot moving both arms towards his body like calling the robot, signaling the robot to walk and follow the person.</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/walking_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/walking_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
        <h3>Walking with Payload</h3>
        <p>Once the human delivers a payload to the robot, he can signal the robot to walk while carrying the payload using a similar movement as the one describe in the "Walk" behaviour. The robot should be able to walk balancing the payload and not letting it fall.</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/walkingPayload_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/walkingPayload_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
    </section>
    
    <section id="Behaviour Blending">
        <h2>Behaviour Blending</h2>
        <p>In this experiment, we aimed to evaluate the model's ability to transition seamlessly between different actions randomly, simulating real-world scenarios where the robot may need to perform various tasks consecutively without explicit instruction.
            We randomly selected a sequence of 30 interactions from our set of scenarios and executed them one after another. The robot autonomously determined how to behave based on its understanding of the environment and the changing context, successfully completing 70% of these scenarios.</p>
        <video controls>
            <source src="https://github.com/ami-iit/xbg/raw/main/assets/videos/blending_low.mov" type="video/mp4">
            <!-- <source src="assets/videos/blending_low.mov" type="video/mp4"> -->
            Your browser does not support the video tag.
        </video>
    </section>

    <section id="conclusion">
        <h2>Conclusion</h2>
        <p>Conclusion content here...</p>
    </section>

    <footer>
        <p>Â© 2024 Artificial and Mechanical Intelligence - Istituto Italiano di Tecnologia. All rights reserved.</p>
    </footer>
</body>
</html>
